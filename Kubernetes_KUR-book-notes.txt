Kubernetes Up and Running - 3rd Edition

kubectl version
This will display two different versions: the version of the local kubectl tool, as well as the version of the Kubernetes API server.

kubectl get componentstatuses --> get a simple diagnostic for the cluster
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-0               Healthy   {"health": "true"}

controller-manager is responsible for running various controllers that regulate behavior in the cluster; for example, ensuring that all of the replicas of a service are available and healthy. The scheduler is responsible for placing different Pods onto different nodes in the cluster. Finally, the etcd server is the storage for the cluster where all of the API objects are stored.

kubectl get nodes

kubectl describe nodes kube1
Name:   kube1
Role:
Labels: beta.kubernetes.io/arch=arm
        beta.kubernetes.io/os=linux
        kubernetes.io/hostname=node-1

You can see that this node is running the Linux OS and is running on an ARM processor.

The Kubernetes proxy is responsible for routing network traffic to load-balanced services in the Kubernetes cluster.

kubectl get daemonSets --namespace=kube-system kube-proxy

Kubernetes uses namespaces to organize objects in the cluster. You can think of each namespace as a folder that holds a set of objects. 

kubectl --namespace=mystuff ..
kubectl --all-namespaces ..

Contexts - to change the default namespace more permanently, you can use a context.

kubectl configuration file located at $HOME/.kube/config

To create a context with different default namespace -
kubectl config set-context my-context --namespace=mystuff

To use this newly created context:
kubectl config use-context my-context

Each Kubernetes object exists at a unique HTTP path; for example, https://your-k8s.com/api/v1/namespaces/default/pods/my-pod leads to the representation of a Pod in the default namespace named my-pod. 

kubectl get <resource-name>
kubectl get <resource-name> <object-name>

-o wide --> gives more detail
-o json, -o yaml  --> To view the complete object in JSON or YAML format

--no-headers

Extracting specific fields from the Object - use JSONPath query language to select fields.

E.g., Extract and print the IP address of the specified Pod:
$ kubectl get pods my-pod -o jsonpath --template={.status.podIP}

kubectl get pods,services
 - display all pods and services for a given namespace.

If you are interested in more detailed information about a particular object, use the describe command

kubectl describe <resource-name> <obj-name>

kubectl explain pods

--watch flag:
to continually observe the state of a particular Kubernetes resource to see changes to the resource when they occur. For example you might be waiting for your application to restart. The --watch flag enables this. You can add this flag to any kubectl get command to continuously monitor the state of a particular resource.

Creating, Updating, and Destroying Kubernetes Objects:

kubectl apply -f obj.yaml

kubectl edit <resource-name> <obj-name>

The apply command also records the history of previous configurations in an annotation within the object. You can manipulate these records with the edit-last-applied, set-last-applied, and view-last-applied commands. For example:

$ kubectl apply -f myobj.yaml view-last-applied
will show you the last state that was applied to the object.

When you want to delete an object, you can simply run:

$ kubectl delete -f obj.yaml

kubectl delete <resource-name> <obj-name>

kubectl will not prompt you to confirm the deletion. Once you issue the command, the object will be deleted.

Labeling and Annotating Objects:
annotate and label commands

kubectl label pods bar color=red

The syntax for annotations is identical.

By default, label and annotate will not let you overwrite an existing label. To do this, you need to add the --overwrite flag.

If you want to remove a label, you can use the <label-name>- syntax:

$ kubectl label pods bar color-

Debugging Commands:

kubectl logs <pod-name>
kubectl logs <pod-name> -c <container-name>

To stream the logs, use -f flag (like tail -f option)
kubectl logs <pod-name> -c <container-name> -f

To execute a command in a running container:
kubectl exec -it <pod-name> -- bash
kubectl exec -it <pod-name> -- env

If you don’t have bash or some other terminal available within your container, you can always attach to the running process:
kubectl attach -it <pod-name>


Copy files from a container to local file system:
$ kubectl cp <pod-name>:</path/to/remote/file> </path/to/local/file>

Copy files from local file system to a container:
$ kubectl cp </path/to/local/file> <pod-name>:</path/to/remote/file>


To forward network traffic from the local machine to the Pod:
$ kubectl port-forward <pod-name> 8080:80
opens up a connection that forwards traffic from the local machine on port 8080 to the remote container on port 80.

You can also use the port-forward command with services by specifying services/<service-name> instead of <pod-name>, but note that if you do port-forward to a service, the requests will only ever be forwarded to a single Pod in that service. They will not go through the service load balancer.

To see a list of the latest 10 events on all objects in a given namespace.
$ kubectl get events

-A --> all namespaces

kubectl top nodes
kubectl top pods
kubectl top pods --all-namespaces

Use kubectl cordon, kubectl drain to safely remove the machine from the cluster. Once the machine is repaired you can use kubectl uncordon

kubectl cordon node node01
kubectl drain node node01
kubctl uncordon node node01

Command Autocompletion:
# Debian/Ubuntu
apt-get install bash-completion

# macOS
brew install bash-completion

# CentOS/Red Hat
yum install bash-completion

source <(kubectl completion bash)
echo "source <(kubectl completion bash)" >> ${HOME}/.bashrc

$ kubectl help
$ kubectl help <command-name>

Pods:
A Pod is a collection of application containers and volumes running in the same execution environment. Pods are the smallest deployable artifact in a Kubernetes cluster (not containers).

kubectl port-forward kuard 8080:8080
a secure tunnel is created from your local machine, through the Kubernetes master, to the instance of the Pod running on one of the worker nodes.
As long as the port-forward command is still running, you can access the Pod at http://localhost:8080.

kubectl logs kuard --previous ==> will get logs from a previous instance of the container. 


Running Commands in Your Container with exec:
$ kubectl exec kuard date

You can also get an interactive session by adding the -it flags:
$ kubectl exec -it kuard ash


Copying Files to and from Containers:
Suppose you had a file called /captures/capture3.txt inside a container in your Pod. You could securely copy that file to your local machine by running:
$ kubectl cp <pod-name>:/captures/capture3.txt ./capture3.txt

Let’s say you want to copy $HOME/config.txt to a remote container. In this case, you can run:
$ kubectl cp $HOME/config.txt <pod-name>:/config.txt

Liveness Probe: Example 4-2. kuard-pod-health.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:blue
      name: kuard
      livenessProbe:
        httpGet:
          path: /healthy
          port: 8080
        initialDelaySeconds: 5
        timeoutSeconds: 1
        periodSeconds: 10
        failureThreshold: 3
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP

The preceding Pod manifest uses an httpGet probe to perform an HTTP GET request against the /healthy endpoint on port 8080 of the kuard container. The probe sets an initialDelaySeconds of 5, and thus will not be called until 5 seconds after all the containers in the Pod are created. The probe must respond within the 1 second timeout, and the HTTP status code must be equal to or greater than 200 and less than 400 to be considered successful. Kubernetes will call the probe every 10 seconds. If more than three consecutive probes fail, the container will fail and restart.

$ kubectl apply -f kuard-pod-health.yaml
$ kubectl port-forward kuard 8080:8080

Point your browser to http://localhost:8080. Click the “Liveness Probe” tab.

Exec probes:
These execute a script or program in the context of the container. Ff this script returns a zero exit code, the probe succeeds; otherwise, it fails. 

CPU & Memory resource requests and limits:
E.g: When you establish limits on a container, the kernel is configured to ensure that consumption cannot exceed these limits.

spec:
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:blue
      name: kuard
      resources:
        requests:
          cpu: "500m"
          memory: "128Mi"
        limits:
          cpu: "1000m"
          memory: "256Mi"


Using Volumes with Pods:
Example 4-5. kuard-pod-vol.yaml
The manifest in the below Example defines a single new volume named kuard-data, which the kuard container mounts to the /data path.

apiVersion: v1
kind: Pod
metadata:
  name: kuard
spec:
  volumes:
    - name: "kuard-data"
      hostPath:
        path: "/var/lib/kuard"
  containers:
    - image: gcr.io/kuar-demo/kuard-amd64:blue
      name: kuard
      volumeMounts:
        - mountPath: "/data"
          name: "kuard-data"
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP

Different Ways of Using Volumes with Pods
- Communication/synchronization:
    emptyDir volume -- is scoped to the Pod’s lifespan, but it can be shared between two containers, forming the basis for communication between our Git sync and web serving containers.
- Cache
- Persistent data
- Mounting the host filesystem


Here is an example of using an NFS server:
...
# Rest of pod definition above here
volumes:
    - name: "kuard-data"
      nfs:
        server: my.nfs.server.local
        path: "/exports"


Applying Labels:

First, create the alpaca-prod deployment and set the ver, app, and env labels:
$ kubectl run alpaca-prod --image=gcr.io/kuar-demo/kuard-amd64:blue --replicas=2 \
  --labels="ver=1,app=alpaca,env=prod"
  
Next, create the alpaca-test deployment and set the ver, app, and env labels with the appropriate values:
$ kubectl run alpaca-test --image=gcr.io/kuar-demo/kuard-amd64:green --replicas=1 \
  --labels="ver=2,app=alpaca,env=test"

Finally, create two deployments for bandicoot. Here we name the environments prod and staging:
$ kubectl run bandicoot-prod --image=gcr.io/kuar-demo/kuard-amd64:green --replicas=2 \
  --labels="ver=2,app=bandicoot,env=prod"
$ kubectl run bandicoot-staging --image=gcr.io/kuar-demo/kuard-amd64:green --replicas=1 \
  --labels="ver=2,app=bandicoot,env=staging"

At this point you should have four deployments alpaca-prod, alpaca-test, bandicoot-prod, and bandicoot-staging:

$ kubectl get deployments --show-labels
NAME                ... LABELS
alpaca-prod         ... app=alpaca,env=prod,ver=1
alpaca-test         ... app=alpaca,env=test,ver=2
bandicoot-prod      ... app=bandicoot,env=prod,ver=2
bandicoot-staging   ... app=bandicoot,env=staging,ver=2

Modifying Labels:

kubectl label deployments alpaca-test "canary=true"

use -L option to show a label value as a column (e.g., CANARY column for label key canary):

$ kubectl get deployments -L canary
NAME                DESIRED   CURRENT   ... CANARY
alpaca-prod         2         2         ... <none>
alpaca-test         1         1         ... true
bandicoot-prod      2         2         ... <none>
bandicoot-staging   1         1         ... <none>


Remove label with dash-suffix: 
kubectl label deployments alpaca-test "canary-"

Using selector:

kubectl get pods --selector="ver=2"
kubectl get pods --selector="app=bandicoot,ver=2"
kubectl get pods --selector="app in (alpaca,bandicoot)"
kubectl get deployments --selector="canary"

Operator	           	Description
---------------------	---------------------------------------
key=value 	  	       	key is set to value
key!=value 	  	       	key is not set to value
key in (value1, value2)	key is one of value1 or value2
key notin (value1, value2)	key is not one of value1 or value2
key                 	key is set
!key                	key is not set

kubectl get deployments --selector='!canary'

You can combine positive and negative selectors:
kubectl get pods -l 'ver=2,!canary'


Label Selectors in API Objects

A selector of app=alpaca,ver in (1, 2) would be converted to this:
selector:
  matchLabels:
    app: alpaca
  matchExpressions:
    - {key: ver, operator: In, values: [1, 2]}


The selector app=alpaca,ver=1 would be represented like this:
selector:
  app: alpaca
  ver: 1


Cleanup:
$ kubectl delete deployments --all
you can use the --selector flag to choose which deployments to delete.


Chapter 7. HTTP Load Balancing with Ingress

Service object operates at Layer 4. This means that it only forwards TCP and UDP connections and doesn’t look inside of those connections.

Kubernetes calls its HTTP-based load-balancing system Ingress.

The Ingress controller is a software system exposed outside the cluster using a service of type: LoadBalancer.  It then proxies requests to “upstream” servers.


Example 7-1. simple-ingress.yaml -- pass all incoming traffic to alpaca service
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: simple-ingress
spec:
  backend:
    serviceName: alpaca
    servicePort: 8080

$ kubectl apply -f simple-ingress.yaml

$ kubectl get ingress
NAME             HOSTS   ADDRESS   PORTS   AGE
simple-ingress   *                 80      13m

$ kubectl describe ingress simple-ingress
Name:             simple-ingress
Namespace:        default
Address:
Default backend:  be-default:8080
(172.17.0.6:8080,172.17.0.7:8080,172.17.0.8:8080)
Rules:
  Host  Path  Backends
  ----  ----  --------
  *     *     be-default:8080 (172.17.0.6:8080,172.17.0.7:8080,172.17.0.8:8080)
...


Using Hostnames:

Example 7-2. host-ingress.yaml - route traffic based on HOST HTTP header
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: host-ingress
spec:
  rules:
  - host: alpaca.example.com
    http:
      paths:
      - backend:
          serviceName: alpaca
          servicePort: 8080

Create this Ingress with kubectl apply:

$ kubectl apply -f host-ingress.yaml
ingress.extensions/host-ingress created

We can verify that things are set up correctly as follows:

$ kubectl get ingress
NAME             HOSTS               ADDRESS   PORTS   AGE
host-ingress     alpaca.example.com            80      54s
simple-ingress   *                             80      13m

$ kubectl describe ingress host-ingress
Name:             host-ingress
Namespace:        default
Address:
Default backend:  default-http-backend:80 (<none>)
Rules:
  Host                Path  Backends
  ----                ----  --------
  alpaca.example.com
                      /   alpaca:8080 (<none>)
Annotations:
  ...
Events:  <none>


Using Paths:
http://bandicoot.example.com --> to bandicoot service
http://bandicoot.example.com/a --> to the alpaca service

Example 7-3. path-ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: path-ingress
spec:
  rules:
  - host: bandicoot.example.com
    http:
      paths:
      - path: "/"
        backend:
          serviceName: bandicoot
          servicePort: 8080
      - path: "/a/"
        backend:
          serviceName: alpaca
          servicePort: 8080

Cleaning Up
To clean up, execute the following:
$ kubectl delete ingress host-ingress path-ingress simple-ingress
$ kubectl delete service alpaca bandicoot be-default
$ kubectl delete deployment alpaca bandicoot be-default


The future of HTTP load balancing for Kubernetes looks to be the Gateway API which is in the development by the Kubernetes special interest group (SIG) dedicated to Networking.

https://gateway-api.sigs.k8s.io/.


Chapter 8. ReplicaSets

Reconciliation Loops
The reconciliation loop is constantly running, observing the current state of the world and taking action to try to make the observed state match the desired state.
The reconciliation loop for ReplicaSets is a single loop, yet it handles user actions to scale up or scale down the ReplicaSet as well as node failures or nodes rejoining the cluster after being absent.

ReplicaSets use label queries to identify the set of Pods they should be managing. 


Finding a ReplicaSet from a Pod
Sometimes you may wonder if a Pod is being managed by a ReplicaSet, and if it is, which one.
To enable this kind of discovery, the ReplicaSet controller adds an ownerReferences section to every Pod that it creates. If you run the following, look for the ownerReferences section:

$ kubectl get pods <pod-name> -o=jsonpath='{.metadata.ownerReferences[0].name}'

 --selector flag or the shorthand -l

$ kubectl get pods -l app=kuard,version=2

Scaling ReplicaSets
kubectl scale replicasets kuard --replicas=4

Horizontal Pod Autoscaling (HPA).
HPA requires the presence of the metrics-server in your cluster. 
You can validate its presence by listing the Pods in the kube-system namespace:

$ kubectl get pods --namespace=kube-system

Autoscaling based on CPU
kubectl autoscale rs kuard --min=2 --max=5 --cpu-percent=80

kubectl get horizontalpodautoscalers
kubectl get hpa

Deleting ReplicaSets

$ kubectl delete rs kuard
replicaset "kuard" deleted
By default, this also deletes the Pods that are managed by the ReplicaSet.
If you don’t want to delete the Pods that the ReplicaSet is managing, you can set the --cascade flag to false to ensure only the ReplicaSet object is deleted and not the Pod

kubectl delete rs kuard --cascade=false


Chapter 9. Deployments

The Deployment object exists to manage the release of new versions. 
Using Deployments, you can simply and reliably roll out new software versions without downtime or errors.


$ kubectl create -f kuard-deployment.yaml

$ kubectl get deployments kuard -o jsonpath --template {.spec.selector.matchLabels}
{"run":"kuard"}

Deployment is managing a ReplicaSet with the label run=kuard

$ kubectl get replicasets --selector=run=kuard
NAME              DESIRED   CURRENT   READY     AGE
kuard-1128242161  1         1         1         13m

$ kubectl scale deployments kuard --replicas=2

$ kubectl get replicasets --selector=run=kuard
NAME              DESIRED   CURRENT   READY     AGE
kuard-1128242161  2         2         2         13m

Scaling the Deployment has also scaled the ReplicaSet it controls.

Now let’s try the opposite, scaling the ReplicaSet:

$ kubectl scale replicasets kuard-1128242161 --replicas=1
replicaset.apps/kuard-1128242161 scaled

Now get that ReplicaSet again:

$ kubectl get replicasets --selector=run=kuard
NAME              DESIRED   CURRENT   READY     AGE
kuard-1128242161  2         2         2         13m

That’s odd. Despite scaling the ReplicaSet to one replica, it still has two replicas as its desired state. What’s going on?

The top-level Deployment object is managing this ReplicaSet. When you adjust the number of replicas to one, it no longer matches the desired state of the Deployment, which has replicas set to 2. The Deployment controller notices this and takes action to ensure the observed state matches the desired state, in this case readjusting the number of replicas back to two.

kubectl rollout
kubectl rollout history -> to obtain the history of rollouts associated with a particular Deployment. If you have a current Deployment in progress, you can use 
kubectl rollout status -> to obtain the current status of a rollout.

Scaling a Deployment

kubectl scale command

Edit your YAML file to increase the number of replicas:
 ...
 spec:
   replicas: 3
 ...

$ kubectl apply -f kuard-deployment.yaml


Updating a Container Image:
Edit the Deployment YAML file and update the container image
 ...
       containers:
       - image: gcr.io/kuar-demo/kuard-amd64:green
         imagePullPolicy: Always
 ...
 
Annotate the template for the Deployment to record some information about the update:

...
spec:
  ...
  template:
    metadata:
      annotations:
        kubernetes.io/change-cause: "Update to green kuard"
...

Make sure you add this annotation to the template and not the Deployment itself, since the kubectl apply command uses this field in the Deployment object. Also, do not update the change-cause annotation when doing simple scaling operations. A modification of change-cause is a significant change to the template and will trigger a new rollout.

Again, you can use kubectl apply to update the Deployment:
$ kubectl apply -f kuard-deployment.yaml

$ kubectl rollout status deployments kuard
deployment "kuard" successfully rolled out

$ kubectl get replicasets -o wide
NAME               DESIRED   CURRENT   READY   ...   IMAGE(S)            ...
kuard-1128242161   0         0         0       ...   gcr.io/kuar-demo/   ...
kuard-1128635377   3         3         3       ...   gcr.io/kuar-demo/   ...

If you are in the middle of a rollout and you can temporarily pause it for some reason using the pause command:

$ kubectl rollout pause deployments kuard
deployment.apps/kuard paused

$ kubectl rollout resume deployments kuard
deployment.apps/kuard resumed

$ kubectl rollout history deployment kuard
deployment.apps/kuard
REVISION  CHANGE-CAUSE
1         <none>
2         Update to green kuard

To view more details about a particular revision, you can add the --revision flag to view details about that specific revision:

$ kubectl rollout history deployment kuard --revision=2
deployment.apps/kuard with revision #2
Pod Template:
  Labels:       pod-template-hash=54b74ddcd4
        run=kuard
  Annotations:  kubernetes.io/change-cause: Update to green kuard
  Containers:
   kuard:
   .....

$ kubectl rollout undo deployments kuard

The undo command works regardless of the stage of the rollout. You can undo both partially completed and fully completed rollouts. An undo of a rollout is actually simply a rollout in reverse.

$ kubectl rollout history deployment kuard
deployment.apps/kuard
REVISION  CHANGE-CAUSE
1         <none>
3         Update to blue kuard
4         Update to green kuard

Revision 2 is missing! It turns out that when you roll back to a previous revision, the Deployment simply reuses the template and renumbers it so that it is the latest revision. What was revision 2 before is now reordered into revision 4.

You can roll back to a specific revision in the history using the --to-revision flag:

$ kubectl rollout undo deployments kuard --to-revision=3
deployment.apps/kuard rolled back

$ kubectl rollout history deployment kuard
deployment.apps/kuard
REVISION  CHANGE-CAUSE
1         <none>
4         Update to green kuard
5         Update to blue kuard

Again, the undo took revision 3, applied it, and renumbered it as revision 5.

Deployment Strategies:
Recreate Strategy
RollingUpdate Strategy

Configuring a rolling update:
There are two parameters:
 - maxUnavailable 
 - maxSurge
they can either be set to an absolute number or to a percentage.

The maxUnavailable parameter sets the maximum number of Pods that can be unavailable during a rolling update. It can either be set to an absolute number (e.g., 3, meaning a maximum of three Pods can be unavailable) or to a percentage (e.g., 20%, meaning a maximum of 20% of the desired number of replicas can be unavailable).


Chapter 10. DaemonSets

A DaemonSet ensures that a copy of a Pod is running across a set of nodes in a Kubernetes cluster. DaemonSets are used to deploy system daemons such as log collectors and monitoring agents, which typically must run on every node.

By default a DaemonSet will create a copy of a Pod on every node unless a node selector is used, which will limit eligible nodes to those with a matching set of labels.

Limiting DaemonSets to Specific Nodes:

Adding Labels to Nodes:
$ kubectl label nodes k0-default-pool-35609c18-z7tb ssd=true

$ kubectl get nodes --selector ssd=true
NAME                            STATUS   ROLES   AGE   VERSION
k0-default-pool-35609c18-z7tb   Ready    agent   1d    v1.21.1

Node Selectors (.spec.template.spec.nodeSelector)- Node selectors can be used to limit what nodes a Pod can run on in a given Kubernetes cluster. Node selectors are defined as part of the Pod spec when creating a DaemonSet. The DaemonSet configuration in Example 10-2 limits NGINX to running only on nodes with the ssd=true label set.

Example 10-2. nginx-fast-storage.yaml
apiVersion: apps/v1
kind: "DaemonSet"
metadata:
  labels:
    app: nginx
    ssd: "true"
  name: nginx-fast-storage
spec:
  selector:
    matchLabels:
      app: nginx
      ssd: "true"
  template:
    metadata:
      labels:
        app: nginx
        ssd: "true"
    spec:
      nodeSelector:
        ssd: "true"
      containers:
        - name: nginx
          image: nginx:1.10.0

$ kubectl apply -f nginx-fast-storage.yaml
daemonset.apps/nginx-fast-storage created

$ kubectl get pods -l app=nginx -o wide
NAME                       READY   STATUS    RESTARTS   AGE   IP            NODE                            NOMINATED NODE   READINESS GATES
nginx-fast-storage-7b90t   1/1     Running   0          44s   10.240.0.48   k0-default-pool-35609c18-z7tb   <none>           <none>

Adding the ssd=true label to additional nodes will cause the nginx-fast-storage Pod to be deployed on those nodes.  The inverse is also true: if a required label is removed from a node, the Pod will be removed by the DaemonSet controller.

Rolling Update of a DaemonSet:
spec.updateStrategy.type=RollingUpdate
When a DaemonSet has an update strategy of RollingUpdate, any change to the spec.template field (or subfields) in the DaemonSet will initiate a rolling update.

There are two parameters that control the rolling update of a DaemonSet:
- spec.minReadySeconds, which determines how long a Pod must be “ready” before the rolling update proceeds to upgrade subsequent Pods
- spec.updateStrategy.rollingUpdate.maxUnavailable, which indicates how many Pods may be simultaneously updated by the rolling update

kubectl rollout status daemonSets my-daemon-set 
will show the current rollout status of a DaemonSet named my-daemon-set.

Deleting a DaemonSet
$ kubectl delete -f fluentd.yaml

Deleting a DaemonSet will also delete all the Pods being managed by that DaemonSet. Set the --cascade flag to false to ensure only the DaemonSet is deleted and not the Pods.

Chapter 11. Jobs

The Job Object - is responsible for creating and managing Pods defined in a template in the job specification.
If the Pod fails before a successful termination, the job controller will create a new Pod based on the Pod template in the job specification.

Two primary attributes of a job:
- the number of job completions and 
- the number of Pods to run in parallel


Type	- Use case	- Behavior	- completions	- parallelism
One shot	- Database migrations	- A single Pod running once until successful termination	1	1
Parallel fixed completions	- Multiple Pods processing a set of work in parallel	One or more Pods running one or more times until reaching a fixed completion count	1+	1+	
Work queue: parallel jobs	- Multiple Pods processing from a centralized work queue	One or more Pods running once until successful termination	1	2+

option 1 - use kubctl run with --restart=OnFailure flag:
$ kubectl run -i oneshot --image=gcr.io/kuar-demo/kuard-amd64:blue \
  --restart=OnFailure --command /kuard 
  -- --keygen-enable \
     --keygen-exit-on-complete \
     --keygen-num-to-gen 10
...
(ID 0) Workload starting
(ID 0 1/10) Item done: SHA256:nAsUsG54XoKRkJwyN+OShkUPKew3mwq7OCc
(ID 0 2/10) Item done: SHA256:HVKX1ANns6SgF/er1lyo+ZCdnB8geFGt0/8
(ID 0 3/10) Item done: SHA256:irjCLRov3mTT0P0JfsvUyhKRQ1TdGR8H1jg
(ID 0 4/10) Item done: SHA256:nbQAIVY/yrhmEGk3Ui2sAHuxb/o6mYO0qRk
(ID 0 5/10) Item done: SHA256:CCpBoXNlXOMQvR2v38yqimXGAa/w2Tym+aI
(ID 0 6/10) Item done: SHA256:wEY2TTIDz4ATjcr1iimxavCzZzNjRmbOQp8
(ID 0 7/10) Item done: SHA256:t3JSrCt7sQweBgqG5CrbMoBulwk4lfDWiTI
(ID 0 8/10) Item done: SHA256:E84/Vze7KKyjCh9OZh02MkXJGoty9PhaCec
(ID 0 9/10) Item done: SHA256:UOmYex79qqbI1MhcIfG4hDnGKonlsij2k3s
(ID 0 10/10) Item done: SHA256:WCR8wIGOFag84Bsa8f/9QHuKqF+0mEnCADY
(ID 0) Workload exiting

There are some things to note here:

-i option - this is an interactive command. kubectl will wait until the job is running and then show the log output from the first Pod in the job.

--restart=OnFailure is the option that tells kubectl to create a Job object.

All of the options after -- are command-line arguments to the container image. These instruct our test server (kuard) to generate 10 4,096-bit SSH keys and then exit.

Note that this job won’t show up in kubectl get jobs unless you pass the -a flag. Without this flag, kubectl hides completed jobs. 
Delete the job before continuing:
$ kubectl delete pods oneshot

Option 2 - create one-shot job using a configuration file, as shown in Example 11-1.

Example 11-1. job-oneshot.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: oneshot
spec:
  template:
    spec:
      containers:
      - name: kuard
        image: gcr.io/kuar-demo/kuard-amd64:blue
        imagePullPolicy: Always
        command:
        - "/kuard"
        args:
        - "--keygen-enable"
        - "--keygen-exit-on-complete"
        - "--keygen-num-to-gen=10"
      restartPolicy: OnFailure
	  
$ kubectl apply -f job-oneshot.yaml
job.batch/oneshot created

$ kubectl describe jobs oneshot
$ kubectl logs oneshot-4kfdt

Job object will automatically pick a unique label and use it to identify the Pods it creates.

Pod failure:

$ kubectl get pod -l job-name=oneshot
NAME            READY     STATUS             RESTARTS   AGE
oneshot-3ddk0   0/1       CrashLoopBackOff   4          3m

$ kubectl delete jobs oneshot


Modify the config file again and change the restartPolicy from OnFailure to Never. Launch this with kubectl apply -f jobs-oneshot-failure2.yaml.

$ kubectl get pod -l job-name=oneshot -a
NAME            READY     STATUS    RESTARTS   AGE
oneshot-0wm49   0/1       Error     0          1m
oneshot-6h9s2   0/1       Error     0          39s
oneshot-hkzw0   1/1       Running   0          6s
oneshot-k5swz   0/1       Error     0          28s
oneshot-m1rdw   0/1       Error     0          19s
oneshot-x157b   0/1       Error     0          57s

By setting restartPolicy: Never we are telling the kubelet not to restart the Pod on failure, but rather just declare the Pod as failed. The Job object then notices and creates a replacement Pod. If you aren’t careful, this’ll create a lot of “junk” in your cluster. For this reason, we suggest you use restartPolicy: OnFailure so failed Pods are rerun in place.

Clean this up with kubectl delete jobs oneshot.

You can use liveness probes with jobs. If the liveness probe policy determines that a Pod is dead, it’ll be restarted or replaced for you.

Parallelism - Example 11-3. job-parallel.yaml

apiVersion: batch/v1
kind: Job
metadata:
  name: parallel
  labels:
    chapter: jobs
spec:
  parallelism: 5
  completions: 10
  template:
    metadata:
      labels:
        chapter: jobs
    spec:
      containers:
      - name: kuard
        image: gcr.io/kuar-demo/kuard-amd64:blue
        imagePullPolicy: Always
        command:
        - "/kuard"
        args:
        - "--keygen-enable"
        - "--keygen-exit-on-complete"
        - "--keygen-num-to-gen=10"
      restartPolicy: OnFailure

$ kubectl apply -f job-parallel.yaml
job.batch/parallel created

Note parallelism and completions parameters.
  parallelism: 5
  completions: 10
10 runs of the job and maximum 5 parallel running jobs at a time.



Work Queues:
kuard has a simple memory-based work queue system built in.

Cleaning up
Using labels, we can clean up all of the stuff we created in this section:
$ kubectl delete rs,svc,job -l chapter=jobs


CronJobs:



